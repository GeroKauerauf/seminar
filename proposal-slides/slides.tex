% This file provides an example Beamer presentation using the RWTH theme
% showcasing some of the more common options, similar to the Powerpoint version
% 12.11.2014: Revision 1 (Harold Bruintjes, Tim Lange)

% For RWTH, beamer should be loaded with class option t (top)
\documentclass[t]{beamer}

% Use fontspec to get Arial font
% Requires use of XeLaTeX
\usepackage{fontspec}
\setmainfont{Arial}
\setsansfont{Arial}
% Also force Arial for math for a more consistent look
\usepackage{unicode-math}

% https://tex.stackexchange.com/questions/426088/texlive-pretest-2018-beamer-and-subfig-collide
\makeatletter
\let\@@magyar@captionfix\relax
\makeatother

% German style date formatting (footer)
\usepackage[ddmmyyyy]{datetime}
\renewcommand{\dateseparator}{.}

\usepackage{MnSymbol,wasysym}

% Format the captions used for figures etc.
\usepackage[compatibility=false]{caption}
\captionsetup{singlelinecheck=off,justification=raggedleft,labelformat=empty,labelsep=none}

% PGFPlots is used for drawing some of the charts
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\input{plot_commands.tex}

% Load the actual RWTH theme. Suggested is to load the full theme,
% as it requires some specific dimensions
\usetheme{rwth}



% ---------------- My Stuff ---------------- %
% ---------- Hyperref ---------- %
\usepackage{hyperref}
\hypersetup{colorlinks,breaklinks,
            urlcolor=[rgb]{0,0.2,0.4},
            linkcolor=[rgb]{0,0.2,0.4}}
\def\UrlBreaks{\do\/\do-}
% Farbe ist darkmidnightblue
% ---------- -------- ---------- %

\begin{document}

\logo{\includegraphics{logo.png}}

% Setup presentation information
\title{Back-Propagation and Algorithms for Training Artificial Neural Networks with TensorFlow}
\date{30. October 2020}
\author{Gero Kauerauf}

\frame{\titlepage}

\section{List of Contents}
\begin{frame}
    \begin{figure}
        \centering
        \begin{itemize}
            \item Introduction
            \item Data Representation
            \item AI Disciplines
            \item Deep Neural Networks
        \end{itemize}
    \end{figure}
\end{frame}


\section{Introduction}
\begin{frame}
    \begin{itemize}
        \item What is a complicated problem for a computer?
        \item A problems that is
        \begin{itemize}
            \item hard to describe formally
            \item intuitive solvable for humans
        \end{itemize}
        \item For example: Recognizing a flower on a picture
        \item Deep Learning
        \begin{itemize}
            \item Hierarchy of Concepts
            \item Representative Graph has Layers
        \end{itemize}
        \item Machine Learning
        \begin{itemize}
            \item Acquiring its own knowledge
            \item Extracting patterns from data
        \end{itemize}
    \end{itemize}
\end{frame}

\section{Data Representation}
\begin{frame}
    \begin{itemize}
        \item Importance of Data Representation
        \begin{itemize}
            \item Tasks can be impossible in one representation and easy in another
            
            \begin{figure}
                \centering
                \begin{minipage}{0.45\textwidth}
                    \centering
                    \includegraphics[width=0.9\textwidth]{../plots/cartesian.pdf} % first figure itself
                \end{minipage}\hfill
                \begin{minipage}{0.45\textwidth}
                    \centering
                    \includegraphics[width=0.9\textwidth]{../plots/polar.pdf} % second figure itself
                \end{minipage}
            \end{figure}

            \item One solution to this is \textbf{representation learning}
            \begin{itemize}
                \item Machine Learning now also discovers the representation itself
                \item Often better Performance
                \item AI can rapidly adapt to new tasks with minimal human intervention
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\section{Different AI disciplines}
\begin{frame}
    \begin{itemize}
        \item Relations between different AI disciplines
        \begin{figure}
            %\centering
            \begin{minipage}{0.45\textwidth}
                \begin{figure}[]
                    \centering
                    \includegraphics[width=\textwidth]{../plots/ai-venn.pdf}
                    %\caption{\href{http://www.deeplearningbook.org}{www.DeepLearningBook.org}}
                \end{figure}
            \end{minipage}\hfill
            \begin{minipage}{0.45\textwidth}
                \begin{figure}[]
                    \centering
                    \includegraphics[width=\textwidth]{../plots/ai-flowchart.pdf}
                    %\caption{\href{http://www.deeplearningbook.org}{www.DeepLearningBook.org}}
                \end{figure}
            \end{minipage}
            \caption{\href{http://www.deeplearningbook.org}{www.DeepLearningBook.org}}
        \end{figure}
    \end{itemize}
\end{frame}

\section{Neural Networks}

\begin{frame}
    \begin{itemize}
        \item An Neural Network consists out of vertices (nodes) and edges
        \item Vertices are modeling Neurons
        \item Edges are modeling synapses
        \item Vertices have an activations function (e.g. a rectifier function)
        \item Each edges has a weights to it
    \end{itemize}

    \begin{itemize}
        \item How it works:
        \begin{itemize}
            \item Input nodes are given an input value
            \item Each nodes in the sums up the inputs that it gets and outputs the activation function value of that sum
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{../plots/activation.pdf}   
    \end{figure}
    \begin{itemize}
        \item Where \(x^T * w + b\) is nothing but \(\sum^{3}_{i=1}(x_i * w_i) + b\)
    \end{itemize}
\end{frame}

\begin{frame}
    \begin{itemize}
        \item A deep neural network consists out of
        \begin{itemize}
            \item an input layer
            \item multiple hidden layers
            \item an output layer
        \end{itemize}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{../plots/nn-2-crop.pdf}
        \end{figure}
    \end{itemize}
\end{frame}

\section{Linear Algebra}
\begin{frame}
    \begin{itemize}
        \item Nothing but linear algebra
        \item All operations can easily be described with vectors and matrices \\
        \item We can describe the weights of the DNN with an weight matrix
        \item The input can be written as an vector, same for the biases
        \item Thus propagating through the network is simply a (scalar-) multiplication of a vector and a column of the matrix plus the corresponding bias
    \end{itemize}
\end{frame}

\section{Computational Graphs}
\begin{frame}
    \begin{figure}
        \centering
        \begin{minipage}{0.45\textwidth}
            \begin{figure}[]
                \centering
                \includegraphics[width=0.5\textwidth]{../plots/computational-graph-c-crop.pdf}
            \end{figure}
        \end{minipage}\hfill
        \begin{minipage}{0.45\textwidth}
            \centering
            \begin{figure}[]
                \includegraphics[width=0.45\textwidth]{../plots/computational-graph-b-crop.pdf}
            \end{figure}
        \end{minipage}
        \caption{\href{http://www.deeplearningbook.org}{www.DeepLearningBook.org}}
    \end{figure}
    \begin{itemize}
        \item A computational graph is used to describe a mathematical expression as an graph
        \item This allows us to apply \emph{graph algorithms} on it
    \end{itemize}
\end{frame}

\begin{frame}
    \begin{figure}
        \centering
        \begin{minipage}{0.45\textwidth}
            \begin{figure}[]
                \centering
                \includegraphics[width=0.7\textwidth]{../plots/computational-graph-c-crop.pdf}
            \end{figure}
        \end{minipage}\hfill
        \begin{minipage}{0.45\textwidth}
            \begin{itemize}
                \item First we compute \(XW\), that we store in \(U^{(1)}\)
                \item Then we calculate \(U^{(2)} = U^{(1)} + b \; \Leftrightarrow \; U^{(2)} = XW + b\)
                \item Finally ReLU refers to \emph{rectifier linear unit}
                \item \(\rightarrow f(x) = max\{0, x\} = \vert x\vert\) \\
                \item This computational graph computes \(H = max\{0, XW + b\}\)
            \end{itemize}
        \end{minipage}
        \caption{\href{http://www.deeplearningbook.org}{www.DeepLearningBook.org}}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{figure}
        \centering
        \begin{minipage}{0.45\textwidth}
            \begin{itemize}
                \item This one here computes \(\hat{y} = \sigma (x^T w + b)\)
            \end{itemize}
        \end{minipage}\hfill
        \begin{minipage}{0.45\textwidth}
            \centering
            \begin{figure}[]
                \includegraphics[width=0.6\textwidth]{../plots/computational-graph-b-crop.pdf}
            \end{figure}
        \end{minipage}
    \end{figure}
\end{frame}

\end{document}