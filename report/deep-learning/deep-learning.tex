\section{Deep learning}
\label{sec:deep-learning}

\emph{Deep learning} refers to the broader family of machine learning algorithms, that are based on artificial neural networks.
Feedforward networks fall into the family of \emph{deep neural networks} (DNNs).

The typical procedure when training a feedforward network is to define a \emph{cost function} for it.
The \emph{cost function} is dependent on the \emph{weights} of the neural network and describes the \emph{error} of the network.
Remember, for a given input \(\fat{x}\) the network computes the output \(\hat{\fat{y}}\).
The cost function now measures the distance of \(\hat{\fat{y}}\) to the real \(\fat{y}\), this distance is called the \emph{error}.
In the context of \emph{supervised learning}, we know \(\fat{y}\), because a human has previously \enquote{classified} the input.
A minimum in the \emph{cost function} therefore minimizes the \emph{error} of the network.
In order to minimize the \emph{cost function} we have to compute its gradient and then \emph{adjust} the weights.
The computation of the gradient is performed by the \emph{back-propagation algorithm}.
The \emph{adjustment} is then performed by an \emph{optimizing algorithm}.