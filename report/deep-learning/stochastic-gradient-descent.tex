\subsection{Stochastic gradient descent}
\label{sec:stochastic-gradient-descent}
\emph{Stochastic gradient descent} (SGD) is the main \emph{optimization algorithm} that is used in \emph{deep learning}.
In \emph{TensorFlow} \cite{tensorflow2015-whitepaper} many of the \emph{optimizers} that come within the deep learning module \emph{keras} are based on the SGD algorithm.
As mentioned earlier, the optimizing algorithm is what \enquote{really} trains the network by updating the weights.

The underlying gradient descent is an iterative optimization algorithm for nonlinear multi-variable vector-valued functions of arbitrary dimension.
Its main idea is to start at an arbitrary point and from there on step into a direction, that minimizes the function value.
This way, it gets closer to a minimum with each step.
Suppose we have a function \(f : \mathbb{R}^2 \rightarrow \mathbb{R}\) that is differentiable.
For any point \(x_0\) we can evaluate its derivative \(f^{\prime}(x_0)\).
And since the derivative describes the slope of the function, it tells us if we have to increase or decrease \(x_0\) in order to make \(f(x_0)\) smaller.

Therefore we can reduce \(f(x)\) by taking a small step into the opposite direction of the derivative \cite{cauchy}, since
\begin{equation}
    \label{eq:cauchy}
    f(x - \epsilon \; \text{sign}(f^{\prime}(x))) < f(x), \qquad \text{for a small enough } \epsilon.
\end{equation}
In the following, we will expand this beyond the \(2\)-dimensional case, so that it can be used for neural networks of arbitrary size.

Let \(x \in \mathbb{R}^n\) and \(f : \mathbb{R}^n \rightarrow \mathbb{R}^m\).
The directional derivative of \(f\) in direction \(u\), a unit vector, is the slope of \(f\) in direction \(u\).
The directional derivative of \(f(x)\) is defined as 
\begin{equation}
    \frac{\partial}{\partial \alpha} f(x + \alpha u) = u^{T} \nabla_x f(x) \big\vert_{\alpha = 0}.
\end{equation}

We would now like to know in which direction \(f\) decreases the fastest.
This can be found out by using the directional derivative.
\begin{align}
      &\min_{u, u^{T}u = 1} u^{T} \nabla_x f(x) \\
    = &\min_{u, u^{T}u = 1} \lVert u \rVert_2 \lVert \nabla_x f(x) \rVert_2 \cos \theta \\
    = &\min_{u, u^{T}u = 1} \lVert \nabla_x f(x) \rVert_2 \cos \theta,
\end{align}
where \(\text{cos } \theta\) is the angle between the gradient and \(u\).
By ignoring the gradient, which does not depend on \(u\), this simplifies further to \(\min_{u} \cos \theta\).
Since \(\cos(\pi) = -1\), this is minimized if the gradient and the unit vector point in opposite directions.
This means the negative gradient points the steepest way downhill.

Therefore taking a step in direction of \(-\nabla_x f(x)\) is an improvement.
It follows that
\begin{equation}
    \lVert f(x - \epsilon \; \nabla_x f(x)) \rVert < \lVert f(x) \rVert, \qquad \text{for a small enough } \epsilon,
\end{equation}
expandes the gradient descent algorithm to arbitrary dimension. This is also called \emph{method of steepest descent}.
Notice that we have to calculate the gradient for each step.
For functions that take a lot of inputs (e.g. DNNs), this is computationally expensive.

\paragraph{Stochastic gradient descent.} 
As its name proposes, this algorithm takes a stochastic approach to avoid the complete calculation of the gradient for functions that can be expressed as a sum.

In the context of training DNNs, we often have \emph{cost functions} that decompose into sums.
We can exploit this, to only calculate the gradient for a partial sum of the cost function.

A \emph{minibatch} is a small subset of training examples, of usually just a few hundred.
We can then calculate the gradient only for the partial cost function defined by this \emph{minibatch}.
By approximating our gradient this way, we may fit a model onto millions of examples way cheaper. 

Let \(Q(w)\) be a function of the form \(Q(w) = \sum^{n}_{i = 1} Q_i(w)\), where we want to estimate \(w\), so that it minimizes \(Q(w)\).

An estimated gradient for a \emph{minibatch} of size \(m\) could then formed as
\begin{equation}
    \boldsymbol{g} = \eta \nabla \sum^{m}_{i = 1} Q_i(w),
\end{equation}
where \(\eta\) describes the step size, in the context of deep learning we call \(\eta\) the \emph{learning rate}.

\begin{algorithm}[H]
    \KwIn{$Q(w) = \sum^{n}_{i = 1} Q_i(w)$}
    \KwOut{$w$ that minimizes $Q(w)$}
    %\BlankLine
    Choose initial $w$\;
    \While{$\lVert Q(w) \rVert \geq \epsilon$}{
        Randomly shuffle dataset entries\;
        \For{$i \gets 1$ \KwTo $n$}{
            $w \gets w - \eta \nabla Q_i(w)$\;
        }
    }
    \caption{Stochastic gradient descent}
\end{algorithm}
The SGD algorithm does not only allow more efficient training, but furthermore to train an existing model, which is possible due to the minibatch procedure.

%\begin{figure}[H]
 %   \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \begin{figure}[H]
            \centering
            \label{fig:six-hump-camel-function}
            \includegraphics*[width=\textwidth]{./deep-learning/six-hump-camel-function-resized.eps}
            \caption{Plot of the six hump camel function: \( f(\fat{x}) = \Big( 4 - 2.1 x^{2}_{1} + \frac{x^{4}_{1}}{3} \Big) x^{2}_{1} + x_{1}x_{2} + (-4 + 4x^{2}_{2}) x^{2}_{2} \)}
        \end{figure}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}        
        In \fref{fig:six-hump-camel-function} we see a nonlinear \(3\)-dimensional function, with global and local minima.
        The difference between gradient descent and its stochastic extension can be easily visualized.
        Gradient descent always finds the shortest, steepest path, whereas SGD takes a longer path with many turns.
        A great analogy is one of a drunken man, who tries to find his way down from a mountain into a valley.
        Gradient descent would then be a sober man.
    \end{minipage}
%\end{figure}