% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\makeatletter
\usepackage[algoruled,boxed,lined]{algorithm2e}
\makeatletter
\g@addto@macro{\@algocf@init}{\SetKwInOut{Parameter}{Parameters}}
\makeatother
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

% -------------------- My Packages -------------------- %
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}

\usepackage{fontspec}
\setmonofont{Roboto Mono}
\usepackage{minted}

%---------- Ich bin ein verdammter KÃ¼nstler ----------
\usepackage{listings}
\definecolor{maroon}{RGB}{128, 0, 0}
\definecolor{pinegreen}{RGB}{1, 121, 111}
\definecolor{darkmidnightblue}{RGB}{0, 51, 102}
\definecolor{rwthblue}{RGB}{0, 84, 159}
% www.colorhexa.com for color references

% Quiet Light
\definecolor{background}{HTML}{f5f5f5}
\definecolor{keyword}{HTML}{4b83cd}
\definecolor{constant}{HTML}{ab6526}
\definecolor{function}{HTML}{aa3731} % bold
\definecolor{comment}{HTML}{aaaaaa} % italic
\definecolor{string}{HTML}{448c27}

% ---------- Hyperref -----------
\hypersetup{colorlinks=true,
            breaklinks=true,
            urlcolor=rwthblue,
            linkcolor=rwthblue,
            citecolor=rwthblue}
\def\UrlBreaks{\do\/\do-}
% -------------------------------

% ------------------ End My Packages ------------------ %

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}
% I don't really care.
\renewcommand\UrlFont{\rmfamily}


\begin{document}
%
\title{Back-Propagation and Algorithms for Training Artificial Neural Networks with TensorFlow}
%
\titlerunning{Back-Propagation and Algorithms for Artificial Neural Networks}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Gero F. Kauerauf}
%
\authorrunning{Kauerauf}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{RWTH Aachen University\\ \email{gero.kauerauf@rwth-aachen.de}} 
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
abstract
\keywords{Deep Learning \and Back-Propagation \and Stochastic Gradient Descent \and TensorFlow}
\end{abstract}
%
%
%

\section{Introduction}
Sample citation \cite{Goodfellow-et-al-2016}.
\section{Problem Description}

\section{Feedforward Networks}
\emph{Deep feedforward networks}, also called \emph{feedforward neural networks} or \emph{multilayer perceptrons} (MLPs) are are basic deep learning models.
Their main purpose is to approximate some kind of function \(f^*(x)\).
Take for example a classification function \(y = f^*(x)\), where every input \(x\) maps to a class \(y\).
A feedforward network now aims to learn the best parameters \(\Theta\) for a function \(y = f(x, \Theta)\) that approximates \(f^*(x)\).

These models are \emph{networks} due to the fact that they can be represented by \emph{directed graphs}.
One of those \emph{graphs} models the consecutive execution of functions whose composition is \(f\).
They are called \emph{feedforward networks} because the input \(x\) flows through these functions without any \emph{feedback} connections.
This means that the graph is \emph{acyclic}.
If the graph is \emph{not acyclic} and therefore \(feedback\) connections exist, we speak of \emph{recurrent neural networks}.

For example let \(f(x) = (f^3 \circ f^2 \circ f^1)(x) = f^3(f^2(f^1(x)))\).
We now define the \emph{depth} of a \emph{feedforward network} \(f\) as the number of functions that compose it.
Our example has a \emph{depth} of \(3\).
These kind of chains are very typical in \emph{neural networks}, as each function in the chain represents a layer in the \emph{directed acyclic graph}.
The last element of the chain (in our example \(f^1\)) and therefore the last layer in the graph is called the \emph{output layer} as it maps the input \(x\) to its final value.
If we think of \(f\) as a classifier function the elements of the \emph{codomain} of \(f^1\) are the classes.

\subsection{Multilayer Perceptrons}
\subsection{tf.keras.Sequential}
\subsection{Cost Functions}

\section{Deep Learning}
\subsection{Computational Graphs}
\subsection{Back-Propagation}
\subsection{Stochastic Gradient Descent}
\subsection{tf.model}

\section{State of the Art}
\section{Conclusion}

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{refs}

\end{document}
